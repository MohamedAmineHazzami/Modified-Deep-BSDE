import torch
import math, time
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class fbsde():
    def __init__(self, x_0, b, sigma, f, g, T, dim_x, dim_y, dim_d):
        self.x_0 = x_0.to(device)
        self.b = b
        self.sigma = sigma
        self.f = f
        self.g = g
        self.T = T
        self.dim_x = dim_x
        self.dim_y = dim_y
        self.dim_d = dim_d


class Model(nn.Module):
    def __init__(self, equation, dim_h, activation: str = "silu", y0_init: float | None = None):
        super().__init__()
        self.linear1 = nn.Linear(equation.dim_x + 1, dim_h)
        self.linear2 = nn.Linear(dim_h, dim_h)
        self.linear3 = nn.Linear(dim_h, equation.dim_y * equation.dim_d)  # tête Z(t,x)
        self.act = (F.silu if activation.lower() == "silu" else F.relu)
        if y0_init is None:
            self.y_0 = nn.Parameter(torch.rand(equation.dim_y, device=device))
        else:
            self.y_0 = nn.Parameter(torch.full((equation.dim_y,), float(y0_init), device=device))
        self.equation = equation

    def _z_from_u(self, u):
        h = self.act(self.linear1(u))
        h = self.act(self.linear2(h))
        return self.linear3(h).reshape(-1, self.equation.dim_y, self.equation.dim_d)

    def forward(self,
                batch_size: int,
                N: int,
                return_aux: bool = False,
                W: torch.Tensor | None = None):
        """
        return_aux=True -> renvoie aussi:
          - res_mse: MSE du résiduel BSDE (consistance locale)
          - z_mse  : MSE Malliavin (cible locale pour Z)
        W: (batch, dim_d, N) permet d'injecter un Brownien (ex. antithétiques)
        """
        dt = self.equation.T / N
        if W is None:
            W = torch.randn(batch_size, self.equation.dim_d, N, device=device) * np.sqrt(dt)
        else:
            assert W.shape == (batch_size, self.equation.dim_d, N)
            W = W.to(device)

        x = self.equation.x_0 + torch.zeros(batch_size, self.equation.dim_x, device=device)
        y = self.y_0 + torch.zeros(batch_size, self.equation.dim_y, device=device)
        ones = torch.ones(batch_size, 1, device=device)

        res_sum = 0.0
        z_mse_sum = 0.0

        for i in range(N):
            t_i = i * dt
            x_i = x  # on fige X_i pour f, sigma et Z
            u = torch.cat((x_i, ones * t_i), dim=1)
            z = self._z_from_u(u)                                   # (B, dim_y, dim_d)
            w = W[:, :, i].unsqueeze(-1)                            # (B, dim_d, 1)
            sig = self.equation.sigma(t_i, x_i)                     # (B, dim_x, dim_d)

            # Next step (schéma explicite)
            x_next = x_i + self.equation.b(t_i, x_i, y) * dt + torch.bmm(sig, w).squeeze(-1)
            f_i = self.equation.f(t_i, x_i, y, z)                   # (B, dim_y)
            y_next = y - f_i * dt + torch.bmm(z, w).squeeze(-1)     # (B, dim_y)

            if return_aux:
                # (1) Résiduel BSDE: r_i = Y_{i+1}-Y_i+f_i dt - Z_i ΔW_i  (→ 0 en théorie)
                r = y_next - y + f_i * dt - torch.bmm(z, w).squeeze(-1)   # (B, dim_y)
                res_sum = res_sum + r.pow(2).mean()

                # (2) Cible Malliavin pour Z:
                #     z_hat_i = ((Y_{i+1} - Y_i + f_i dt) * ΔW_i^T) / dt   (non biaisé en espérance)
                dYf = (y_next.detach() - y.detach() + f_i.detach() * dt)   # (B, dim_y)
                z_hat = (dYf.unsqueeze(2) * w.transpose(1, 2)) / dt        # (B, dim_y, dim_d)
                z_mse_sum = z_mse_sum + (z - z_hat).pow(2).mean()

            x, y = x_next, y_next

        if return_aux:
            return x, y, res_sum / N, z_mse_sum / N
        return x, y

    
    def z_field(self, x, t):
        """
        x: (batch, dim_x); t: scalaire, (batch,) ou (batch,1)
        -> (batch, dim_y, dim_d)
        """
        if not torch.is_tensor(t):
            t = torch.tensor(t, device=device, dtype=x.dtype)
        if t.dim() == 0:
            t = t.view(1, 1).expand(x.size(0), 1)
        elif t.dim() == 1:
            t = t.view(-1, 1)
        u = torch.cat((x.to(device), t.to(device)), dim=1)
        return self._z_from_u(u)

    @torch.no_grad()
    def predict_z(self, x, t):
        self.eval()
        return self.z_field(x, t)


class BSDEsolver():
    def __init__(self, equation, dim_h, activation: str = "silu", y0_init: float | None = None):
        self.model = Model(equation, dim_h, activation=activation, y0_init=y0_init).to(device)
        self.equation = equation

    def train(self,
              batch_size: int,
              N: int,
              itr: int,
              log: bool,
              lambda_res: float = 0.0,
              lambda_z: float = 0.0,
              microbatch: int | None = None,
              use_amp: bool | None = None,
              clip_grad: float | None = 1.0,
              print_every: int | None = 200):
        """
        - lambda_res : poids de la pénalité de consistance BSDE (résiduel local)
        - lambda_z   : poids de la pénalité Malliavin (cible locale pour Z)
        - microbatch : fractionne batch_size pour éviter l'OOM (batch effectif conservé)
        - use_amp    : precision mixte (auto=True si GPU)
        - clip_grad  : clipping des gradients (None = off)
        - print_every: fréquence d'affichage (None = silence)
        """
        criterion = torch.nn.MSELoss().to(device)
        optimizer = torch.optim.Adam(self.model.parameters())

        if use_amp is None:
            use_amp = torch.cuda.is_available()
        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)

        loss_data, y0_data = [], []

        def _step_grads():
            if clip_grad is not None:
                clip_grad_norm_(self.model.parameters(), max_norm=clip_grad)
            scaler.step(optimizer); scaler.update()

        for it in range(itr):
            self.model.train()
            optimizer.zero_grad(set_to_none=True)

            if microbatch is None:
                with torch.cuda.amp.autocast(enabled=use_amp):
                    xT, yT, res_mse, z_mse = self.model(
                        batch_size, N, return_aux=(lambda_res > 0.0 or lambda_z > 0.0)
                    )
                    loss_T = criterion(self.equation.g(xT), yT)
                    loss = loss_T \
                           + (lambda_res * res_mse if lambda_res > 0.0 else 0.0) \
                           + (lambda_z   * z_mse   if lambda_z   > 0.0 else 0.0)
                scaler.scale(loss).backward()
                _step_grads()
                total_loss = loss.detach().item()

            else:
                done = 0
                total_loss = 0.0
                while done < batch_size:
                    mb = min(microbatch, batch_size - done)
                    with torch.cuda.amp.autocast(enabled=use_amp):
                        xT, yT, res_mse, z_mse = self.model(
                            mb, N, return_aux=(lambda_res > 0.0 or lambda_z > 0.0)
                        )
                        loss_T = criterion(self.equation.g(xT), yT)
                        loss = loss_T \
                               + (lambda_res * res_mse if lambda_res > 0.0 else 0.0) \
                               + (lambda_z   * z_mse   if lambda_z   > 0.0 else 0.0)
                        loss_w = loss * (mb / batch_size)  # pondération micro-batch
                    scaler.scale(loss_w).backward()
                    total_loss += loss.detach().item() * (mb / batch_size)
                    done += mb
                _step_grads()

            if log:
                loss_data.append(total_loss)
                y0_data.append(self.model.y_0.detach().mean().item())

            if print_every is not None and (it + 1) % print_every == 0:
                extra = []
                if lambda_res > 0.0: extra.append(f"λ_res={lambda_res}")
                if lambda_z   > 0.0: extra.append(f"λ_z={lambda_z}")
                msg = f"[{it+1}/{itr}] loss={total_loss:.4e}"
                if extra: msg += " (" + ", ".join(extra) + ")"
                print(msg)

        if log:
            np.save('loss_data.npy', np.array(loss_data, dtype=np.float64))
            np.save('y0_data.npy', np.array(y0_data, dtype=np.float64))









print("CUDA:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# ==== Problème (10D) ====
dim_x, dim_y, dim_d = 10, 1, 10
T = 1.0
x_0 = torch.zeros(dim_x, device=device)

# Poids de g: 2 sur la 1ère coordonnée, 1 sur les autres
w = torch.ones(dim_x, device=device); w[0] = 2.0
# Baseline analytique pour α=0 : E[g(X_T)] = 1/2 * sum_j w_j
E_g_alpha0 = 0.5 * (w.sum().item())   # = 5.5

# Diffusion identité
I = torch.eye(dim_x, dim_d, device=device).unsqueeze(0)  # (1, d, d)
def b(t, x, y):    return torch.zeros(x.size(0), dim_x, device=device)
def sigma(t, x):   return I.expand(x.size(0), -1, -1)

# α sera mis à jour dynamiquement par set_alpha(...)
ALPHA = 0.0
def f(t, x, y, z):
    z_max = torch.max(z, dim=2).values  # (batch, 1)
    return ALPHA * z_max

def g(x):
    return ((x >= 0).float() * w).sum(dim=1, keepdim=True)

# ==== Fabrique solver ====
def make_solver(y0_init=None, dim_h=256):
    eq = fbsde(x_0, b, sigma, f, g, T, dim_x, dim_y, dim_d)
    return BSDEsolver(eq, dim_h=dim_h, activation="silu", y0_init=y0_init)

# ==== Helpers ====
def set_alpha(a: float):
    global ALPHA
    ALPHA = float(a)

@torch.no_grad()
def read_y0_and_Z0(solver):
    y0 = float(solver.model.y_0.detach().cpu())
    x0 = solver.equation.x_0.view(1, -1)
    Z0 = solver.model.predict_z(x0, t=0.0).squeeze(0).cpu().numpy()
    return y0, Z0

def train_block(solver, alpha, N, itr, batch, lambda_res=0.02, lambda_z=0.05,
                microbatch=1024, use_amp=True, tag=""):
    set_alpha(alpha)
    t0 = time.perf_counter()
    solver.train(batch_size=batch, N=N, itr=itr, log=False,
                 lambda_res=lambda_res, lambda_z=lambda_z,
                 microbatch=microbatch, use_amp=use_amp,
                 clip_grad=1.0, print_every=200)
    if torch.cuda.is_available(): torch.cuda.synchronize()
    dt = time.perf_counter() - t0
    y0, Z0 = read_y0_and_Z0(solver)
    print(f"[{tag}] α={alpha}, N={N}, itr={itr}, batch={batch}, λ_res={lambda_res}, λ_Z={lambda_z}  -> {dt:.1f}s")
    print(f"     y0 ≈ {y0:.6f}")
    return y0





# =================
# 1) Courbe y0(α) 
# =================

# Grille d'alphas
alphas = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]

# Créer un solver warm-starté sur α=0
solver = make_solver(y0_init=E_g_alpha0, dim_h=256)

y0_of_alpha = {}

# Warm-start α=0 (stabilise Z, met y0≈E[g])
y0_warm = train_block(solver, alpha=0.0, N=120, itr=1000, batch=4096,
                      lambda_res=0.02, lambda_z=0.05,
                      microbatch=1024, use_amp=True, tag="warm α=0")
y0_of_alpha[0.0] = y0_warm

# Balayage α>0 : pour chaque α, enchaîner N=120→160→200 et décroître λ
for a in alphas:
    if a == 0.0: 
        continue
    y0_a_1 = train_block(solver, alpha=a, N=120, itr=800, batch=4096,
                         lambda_res=0.02, lambda_z=0.05,
                         microbatch=1024, use_amp=True, tag=f"α={a}")
    y0_a_2 = train_block(solver, alpha=a, N=160, itr=800, batch=4096,
                         lambda_res=0.015, lambda_z=0.03,
                         microbatch=1024, use_amp=True, tag=f"α={a}")
    y0_a_3 = train_block(solver, alpha=a, N=200, itr=1000, batch=4096,
                         lambda_res=0.01, lambda_z=0.02,
                         microbatch=1024, use_amp=True, tag=f"α={a}")
    y0_of_alpha[a] = y0_a_3

#on continue avec d'autres valeurs d'alpha:
alphas = [3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0,7.5,8.0]
for a in alphas:
    y0_a_1 = train_block(solver, alpha=a, N=120, itr=800, batch=4096,
                         lambda_res=0.02, lambda_z=0.05,
                         microbatch=1024, use_amp=True, tag=f"α={a}")
    y0_a_2 = train_block(solver, alpha=a, N=160, itr=800, batch=4096,
                         lambda_res=0.015, lambda_z=0.03,
                         microbatch=1024, use_amp=True, tag=f"α={a}")
    y0_a_3 = train_block(solver, alpha=a, N=200, itr=1000, batch=4096,
                         lambda_res=0.01, lambda_z=0.02,
                         microbatch=1024, use_amp=True, tag=f"α={a}")
    y0_of_alpha[a] = y0_a_3

alphas = [8.5, 9.0, 9.5, 10.0]
for a in alphas:
    y0_a_1 = train_block(solver, alpha=a, N=120, itr=800, batch=4096,
                         lambda_res=0.02, lambda_z=0.05,
                         microbatch=1024, use_amp=True, tag=f"α={a}")
    y0_a_2 = train_block(solver, alpha=a, N=160, itr=800, batch=4096,
                         lambda_res=0.015, lambda_z=0.03,
                         microbatch=1024, use_amp=True, tag=f"α={a}")
    y0_a_3 = train_block(solver, alpha=a, N=200, itr=1000, batch=4096,
                         lambda_res=0.01, lambda_z=0.02,
                         microbatch=1024, use_amp=True, tag=f"α={a}")
    y0_of_alpha[a] = y0_a_3

# Tri et affichage
alphas_sorted = sorted(y0_of_alpha.keys())
alphas_sorted.remove(11)
y0_vals = [y0_of_alpha[a] for a in alphas_sorted]

print("\n=== Tableau y0(α) ===")
for i in range(len(alphas_sorted)):
    print(f"α={alphas_sorted[i]:>4.1f}  ->  y0≈ {y0_vals[i]:.6f}")

# Figure y0 vs α
plt.figure()
plt.plot(alphas_sorted, y0_vals, marker='o')
plt.xlabel(r'$\alpha - \beta$')
plt.ylabel(r'$J(0,0,0,\alpha ,\beta)$')
plt.title('Expected gain by budget difference (10D)')
plt.grid(True)
plt.show()
