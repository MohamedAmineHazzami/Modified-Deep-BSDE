import torch

import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.nn.utils import clip_grad_norm_

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


class fbsde():
    def __init__(self, x_0, b, sigma, f, g, T, dim_x, dim_y, dim_d):
        self.x_0 = x_0.to(device)
        self.b = b
        self.sigma = sigma
        self.f = f
        self.g = g
        self.T = T
        self.dim_x = dim_x
        self.dim_y = dim_y
        self.dim_d = dim_d


class Model(nn.Module):
    def __init__(self, equation, dim_h, activation: str = "silu", y0_init: float | None = None):
        super().__init__()
        self.linear1 = nn.Linear(equation.dim_x + 1, dim_h)
        self.linear2 = nn.Linear(dim_h, dim_h)
        self.linear3 = nn.Linear(dim_h, equation.dim_y * equation.dim_d)  # tête Z(t,x)
        self.act = (F.silu if activation.lower() == "silu" else F.relu)
        if y0_init is None:
            self.y_0 = nn.Parameter(torch.rand(equation.dim_y, device=device))
        else:
            self.y_0 = nn.Parameter(torch.full((equation.dim_y,), float(y0_init), device=device))
        self.equation = equation

    def _z_from_u(self, u):
        h = self.act(self.linear1(u))
        h = self.act(self.linear2(h))
        return self.linear3(h).reshape(-1, self.equation.dim_y, self.equation.dim_d)

    def forward(self,
                batch_size: int,
                N: int,
                return_aux: bool = False,
                W: torch.Tensor | None = None):
        """
        return_aux=True -> renvoie aussi:
          - res_mse: MSE du résiduel BSDE (consistance locale)
          - z_mse  : MSE Malliavin (cible locale pour Z)
        W: (batch, dim_d, N) permet d'injecter un Brownien
        """
        dt = self.equation.T / N
        if W is None:
            W = torch.randn(batch_size, self.equation.dim_d, N, device=device) * np.sqrt(dt)
        else:
            assert W.shape == (batch_size, self.equation.dim_d, N)
            W = W.to(device)

        x = self.equation.x_0 + torch.zeros(batch_size, self.equation.dim_x, device=device)
        y = self.y_0 + torch.zeros(batch_size, self.equation.dim_y, device=device)
        ones = torch.ones(batch_size, 1, device=device)

        res_sum = 0.0
        z_mse_sum = 0.0

        for i in range(N):
            t_i = i * dt
            x_i = x  # on fige X_i pour f, sigma et Z
            u = torch.cat((x_i, ones * t_i), dim=1)
            z = self._z_from_u(u)                                   # (B, dim_y, dim_d)
            w = W[:, :, i].unsqueeze(-1)                            # (B, dim_d, 1)
            sig = self.equation.sigma(t_i, x_i)                     # (B, dim_x, dim_d)

            # Next step (schéma explicite)
            x_next = x_i + self.equation.b(t_i, x_i, y) * dt + torch.bmm(sig, w).squeeze(-1)
            f_i = self.equation.f(t_i, x_i, y, z)                   # (B, dim_y)
            y_next = y - f_i * dt + torch.bmm(z, w).squeeze(-1)     # (B, dim_y)

            if return_aux:
                # (1) Résiduel BSDE: r_i = Y_{i+1}-Y_i+f_i dt - Z_i ΔW_i  (→ 0 en théorie)
                r = y_next - y + f_i * dt - torch.bmm(z, w).squeeze(-1)   # (B, dim_y)
                res_sum = res_sum + r.pow(2).mean()

                # (2) Cible Malliavin pour Z:
                #     z_hat_i = ((Y_{i+1} - Y_i + f_i dt) * ΔW_i^T) / dt   (non biaisé en espérance)
                dYf = (y_next.detach() - y.detach() + f_i.detach() * dt)   # (B, dim_y)
                z_hat = (dYf.unsqueeze(2) * w.transpose(1, 2)) / dt        # (B, dim_y, dim_d)
                z_mse_sum = z_mse_sum + (z - z_hat).pow(2).mean()

            x, y = x_next, y_next

        if return_aux:
            return x, y, res_sum / N, z_mse_sum / N
        return x, y

    
    def z_field(self, x, t):
        """
        x: (batch, dim_x); t: scalaire, (batch,) ou (batch,1)
        -> (batch, dim_y, dim_d)
        """
        if not torch.is_tensor(t):
            t = torch.tensor(t, device=device, dtype=x.dtype)
        if t.dim() == 0:
            t = t.view(1, 1).expand(x.size(0), 1)
        elif t.dim() == 1:
            t = t.view(-1, 1)
        u = torch.cat((x.to(device), t.to(device)), dim=1)
        return self._z_from_u(u)

    @torch.no_grad()
    def predict_z(self, x, t):
        self.eval()
        return self.z_field(x, t)


class BSDEsolver():
    def __init__(self, equation, dim_h, activation: str = "silu", y0_init: float | None = None):
        self.model = Model(equation, dim_h, activation=activation, y0_init=y0_init).to(device)
        self.equation = equation

    def train(self,
              batch_size: int,
              N: int,
              itr: int,
              log: bool,
              lambda_res: float = 0.0,
              lambda_z: float = 0.0,
              microbatch: int | None = None,
              use_amp: bool | None = None,
              clip_grad: float | None = 1.0,
              print_every: int | None = 200):
        """
        - lambda_res : poids de la pénalité de consistance BSDE (résiduel local)
        - lambda_z   : poids de la pénalité Malliavin (cible locale pour Z)
        - microbatch : fractionne batch_size pour éviter l'OOM (batch effectif conservé)
        - use_amp    : precision mixte (auto=True si GPU)
        - clip_grad  : clipping des gradients (None = off)
        - print_every: fréquence d'affichage (None = silence)
        """
        criterion = torch.nn.MSELoss().to(device)
        optimizer = torch.optim.Adam(self.model.parameters())

        if use_amp is None:
            use_amp = torch.cuda.is_available()
        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)

        loss_data, y0_data = [], []

        def _step_grads():
            if clip_grad is not None:
                clip_grad_norm_(self.model.parameters(), max_norm=clip_grad)
            scaler.step(optimizer); scaler.update()

        for it in range(itr):
            self.model.train()
            optimizer.zero_grad(set_to_none=True)

            if microbatch is None:
                with torch.cuda.amp.autocast(enabled=use_amp):
                    xT, yT, res_mse, z_mse = self.model(
                        batch_size, N, return_aux=(lambda_res > 0.0 or lambda_z > 0.0)
                    )
                    loss_T = criterion(self.equation.g(xT), yT)
                    loss = loss_T \
                           + (lambda_res * res_mse if lambda_res > 0.0 else 0.0) \
                           + (lambda_z   * z_mse   if lambda_z   > 0.0 else 0.0)
                scaler.scale(loss).backward()
                _step_grads()
                total_loss = loss.detach().item()

            else:
                done = 0
                total_loss = 0.0
                while done < batch_size:
                    mb = min(microbatch, batch_size - done)
                    with torch.cuda.amp.autocast(enabled=use_amp):
                        xT, yT, res_mse, z_mse = self.model(
                            mb, N, return_aux=(lambda_res > 0.0 or lambda_z > 0.0)
                        )
                        loss_T = criterion(self.equation.g(xT), yT)
                        loss = loss_T \
                               + (lambda_res * res_mse if lambda_res > 0.0 else 0.0) \
                               + (lambda_z   * z_mse   if lambda_z   > 0.0 else 0.0)
                        loss_w = loss * (mb / batch_size)  # pondération micro-batch
                    scaler.scale(loss_w).backward()
                    total_loss += loss.detach().item() * (mb / batch_size)
                    done += mb
                _step_grads()

            if log:
                loss_data.append(total_loss)
                y0_data.append(self.model.y_0.detach().mean().item())

            if print_every is not None and (it + 1) % print_every == 0:
                extra = []
                if lambda_res > 0.0: extra.append(f"λ_res={lambda_res}")
                if lambda_z   > 0.0: extra.append(f"λ_z={lambda_z}")
                msg = f"[{it+1}/{itr}] loss={total_loss:.4e}"
                if extra: msg += " (" + ", ".join(extra) + ")"
                print(msg)

        if log:
            np.save('loss_data.npy', np.array(loss_data, dtype=np.float64))
            np.save('y0_data.npy', np.array(y0_data, dtype=np.float64))
